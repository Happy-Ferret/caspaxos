\documentclass[12pt]{article}

\usepackage[toc,page]{appendix}

\usepackage[utf8]{inputenc}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{enumerate}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\small}

\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{Liberation Serif}
\setsansfont{Liberation Sans}
\setmonofont{Liberation Mono}

\usepackage{mathtools}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\usepackage{polyglossia}
\setdefaultlanguage{english}

\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref}

\begin{document}

\title{CASPaxos: Replicated State Machines without logs}
\date{30 January 2018}
\author{Denis Rystsov \\
\href{mailto:rystsov.denis@gmail.com}{rystsov.denis@gmail.com}}


\maketitle

\begin{abstract}
CASPaxos is a replicated state machine (RSM) protocol, an extension of Single Decree Paxos (Synod) protocol. Unlike Raft and Multi-Paxos, it doesn't use leader election and log replication, so it avoids associated complexity such as log compaction and doesn't have latency issues when a leader crashes.

The lightweight nature of CASPaxos creates new applications for RSM, e.g. instead of representing a key/value storage as a single RSM; it's possible to run an instance of CASPaxos per key to better fault tolerance and better performace on multi-core systems.

This paper describes CASPaxos protocol, formally proves its safety properties, covers cluster membership change and evaluates a CASPaxos-based key/value storage.
\end{abstract}

\section{Introduction}

Multi-Paxos\cite{lamport01} and Raft\cite{raft} protocols allow a collection of machines to work as a state machine tolerating failures and network issues. The protocols preserve liveness when at least $\floor{N/2} + 1$ of $N$ machines are up and connected and preserve safety in the presence of arbitrary crash/recovery and loss of messages.

The problem of keeping RSM work when its nodes crach and network is falling apart is isomorphic to the problem of master-master replication of a linearizable distributed storage under the same conditions. So those protocols are widely used in the industry as a foundation of such databases as Chubby\cite{chubby}, Etcd\footnote{\href{https://github.com/coreos/etcd}{https://github.com/coreos/etcd}}, Spanner\cite{spanner}, etc.

Despite the wide adoption, there are a lot of indications that the protocols are complex and cause availability problem when a leader crashes. Diego Ongaro and John Ousterhout write in "In Search of an Understandable Consensus Algorithm"\cite{raft}:

\begin{quote}
In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers. We struggled with Paxos ourselves; we were not able to understand the complete protocol until after reading several simplified explanations and designing our own alternative protocol, a process that took almost a year
\end{quote}

Google's engineers write about their experience of building a Paxos-based database in the "Paxos Made Live"\cite{chubby} paper:

\begin{quote}
Despite the existing literature in the field, building such a database proved to be non-trivial \ldots{} While Paxos can be described with a page of pseudo-code, our complete implementation contains several thousand lines of C++ code \ldots{} There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.
\end{quote}

The complexity of RSM protocols may lead to errors in implementations. Kyle Kingsbury made a comprehensive research\footnote{\href{https://aphyr.com/tags/jepsen}{https://aphyr.com/tags/jepsen}} of distributed consistent databases and found violations of linearizability in almost every database he tested including MongoDB, Etcd, Consul, RethinkDB, VoltDB and CockroachDB.

The "There Is More Consensus in Egalitarian Parliaments" paper\cite{epaxos} descibes the negative implications of leader-based system which are applicable both to Multi-Paxos and Raft:

\begin{quote}
Traditional Paxos variants are sensitive to both long-term and transient load spikes and network delays that increase latency at the master \ldots{} this single-master optimization can harm availability: if the master fails, the system cannot service requests until a new master is elected \ldots{} Multi-Paxos has high latency because the local replica must forward all commands to the stable leader.
\end{quote}

{\bf Contributions.} I present CASPaxos, a novel protocol for building RSM that avoids complexities of log based systems.

Multi-Paxos is RSM built on top of a replicated log which treats every log entry as a command. The replicated log is modelled as an array of Synod instances. According to D. Ongaro and J. Ousterhout, its complexity comes from the composition rules:

\begin{quote}
We hypothesize that Paxosâ€™ opaqueness derives from its choice of the single-decree subset as its foundation \ldots{} The composition rules for Multi-Paxos add significant additional complexity and subtlety.

One reason is that there is no widely agreed upon algorithm for multi-Paxos. Lamportâ€™s descriptions are mostly about single-decree Paxos; he sketched possible approaches to multi-Paxos, but many details are missing. As a result, practical systems bear little resemblance to Paxos. Each implementation begins with Paxos, discovers the difficulties in implementing it, and then develops a significantly different architecture \ldots{} real implementations are so different from Paxos that the proofs have little value
\end{quote}

CASPaxos extends Synod, a write-once distributed register, into a rewritable distributed register (which is isomorphic to a RSM) instead of using it as a building block, so there is no composition and the associated complexity.

According to an experimental study\cite{raft} and the number of open source implementations\footnote{\href{https://raft.github.io/\#implementations}{https://raft.github.io/\#implementations}} Raft succeeded in its goal to be understandable. However, its complexity is still comparable with Multi-PAxos: both systems\cite{chubby}\cite{raft} have several thousand of lines of code, both use concepts of leader election and leases, both are based on logs and require log compaction. CASPaxos is significantly simpler: it doesn't have those pieces and its implementation\footnote{\href{https://github.com/gryadka/js}{https://github.com/gryadka/js}} is less than 500 lines of code.

Being just an extension of Synod, CASPaxos uses its symmetric peer-to-peer approach and automatically achieves the goals set in the EPaxos\cite{epaxos} paper: (1) optimal commit latency in the wide-area when tolerating one and two failures, under realistic conditions; (2) uniform load balancing across all replicas (thus achieving high throughput); and (3) graceful performance degradation when replicas are slow or crash.

The formal proof is included into the appendix, Tobias Schottdorf\footnote{\href{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}} and Greg Rogers\footnote{\href{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}} model checked protocol with TLA+ models, and an implementation was tested with faults injection methodology.

In the following sections, I describe the CASPaxos protocol, cluster membership change and evaluate a CASPaxos-based key/value storage.

\section{Algorithm}

We begin by briefly describing the Synod protocol from the perspective of master-master replication, followed by an overview of its extension into CASPaxos.

\subsection{Synod}

An implementation of the Synod protocol is a distributed register which a client can initialize only once. If several clients try to initialize a register concurrently, then the requests either prevent each other from continuing, or a single initialization succeeds. Once a client receives confirmation, all the follow-up initializations must result in a conflict and return the accepted value.

The system belongs to the CP category of the CAP theorem and keeps working without compromising safety when at least $\floor{N/2} + 1$ of $N$ nodes are up and connected; with more failures, it gives up availability.

The roles of nodes in the system are:
\begin{enumerate}
  \item {\bf Clients} initiate request by communicating with a proposer; clients may be stateless, the system may have arbitrary numbers of clients.
  \item {\bf Proposers} perform the initialization by communicating with acceptors. Proposers keep state to generate unique increasing update ID (ballot number), the system may have arbitrary numbers of proposers.
  \item {\bf Acceptors} store the accepted value, the system should have $2F+1$ acceptor to tolerate $F$ failures.
\end{enumerate}

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[y=-1cm]
    \node at (0,-0.5)[scale=0.8]{Client};
    \node at (2,-0.5)[scale=0.8]{Proposer};
    \node at (4.5,-0.5)[scale=0.8]{Acceptor A};
    \node at (6.5,-0.5)[scale=0.8]{Acceptor B};
    \node at (8.5,-0.5)[scale=0.8]{Acceptor C};
    
    \draw (0,0) -- (0,5.6);
    \draw (2,0) -- (2,5.6);
    \draw (4.5,0) -- (4.5,5.6);
    \draw (6.5,0) -- (6.5,2.05);
    \draw (8.5,0) -- (8.5,5.6);
    \draw (6.4,2.05) -- (6.6,2.05);
  
    \draw[dotted] (-0.2,0.75) -- (8.6,0.75);
    \draw[dotted] (-0.2,2.55) -- (8.6,2.55);
    \draw[dotted] (-0.2,4.05) -- (8.6,4.05);
  
    \begin{scope}[very thick]
      \draw[->] (0,0.5) -- (2,0.5) node[above, midway, scale=0.8]{set 3};
      
      \node at (1,1.6)[scale=0.8]{Propose};
      \draw[->] (2,1) -- (4.5,1);
      \draw[->] (2,1.3) -- (6.5,1.3);
      \draw[->] (2,1.6) -- (8.5,1.6);
      \draw[<-, dashed] (2,1.9) -- (4.5,1.9);
      \draw[<-, dashed] (2,2.2) -- (8.5,2.2);
  
      \node at (1,3.35)[scale=0.8]{Accept};
      \draw[->] (2,2.9) -- (4.5,2.9);
      \draw[->] (2,3.2) -- (8.5,3.2);
      \draw[<-, dashed] (2,3.5) -- (4.5,3.5);
      \draw[<-, dashed] (2,3.8) -- (8.5,3.8);
  
      \draw[<-, dashed] (0,4.3) -- (2,4.3) node[below, midway, scale=0.8]{ok};
    \end{scope}
  \end{tikzpicture}
\end{figure}

The initialization procedure:
\begin{enumerate}
  \item A client proposes a value to a proposer.
  \item The proposer generates a ballot number, $B$, and send a prepare message containing that number to the acceptors.
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Persists the ballot number as a promise and returns a confirmation either with an empty value (if it hasn't accepted any yet) or with a tuple of an accepted value and its ballot number.
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \begin{itemize}
    \item If they all contain an empty value then the proposer writes "ok" to the result variable and sends an accept message containing the ballot number $B$ and the proposing value to the acceptors.
    \item If at least one message contains a tuple then the proposer picks the tuple with the highest ballot number, writes ("conflict", the tuple's value) to the result variable and sends an accept message with the generated ballot number $B$ and the tuple's value to the acceptors.
  \end{itemize}
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Erases the promise, marks the received tuple (ballot number, value) as the accepted value and returns a confirmation
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \item The proposer returns the result variable to the client
\end{enumerate}

It's convient to use tuples as ballot numbers, to generate it a proposer combines an increasing counter with its numerical ID: (counter, ID). To compare ballot tuples, we should compare the counter parts and use ID only as a tiebreaker. When a proposer receives a conflict from a acceptor, it should fast-forward its counter to avoid a conflict on the next request.

\subsection{CASPaxos}

CASPaxos is a rewritable distributed register. Clients change its state by submitting side effect free functions which take the current state as an argument and yield new state as a result. Out of the concurrent requests only one can succeed, once a client gets a confirmation of the change it's guaranteed that all future states are its descendants: there exists a chain of changes linking them together.

Just like Synod, it's a CP-system, and it requires $2F+1$ nodes to tolerate $F$ failures. Also it uses the same roles: clients, proposers and acceptors, and a very similar two phase state transition mechanism:
\begin{enumerate}
  \item A client submits a change function to a proposer.
  \item The proposer generates a ballot number, $B$, and send a prepare message containing that number to the acceptors.
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Persists the ballot number as a promise and returns a confirmation either with an empty value (if it doesn't have any state yet) or with a tuple of the latest value and its ballot number.
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \begin{itemize}
    \item The proposer defines the current state as $\emptyset$ if each confirmation contains an empty value; if at least one confirmation contains a tuple, then the proposer picks the value of the tuple with the highest ballot number as the current state.
    \item Then it invokes the change function passing the current state as an argument to calculate the new state and sends it with the generated ballot number $B$ as an accept message to the acceptors.
  \end{itemize}
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Erases the promise, marks the received tuple (ballot number, value) as the accepted value and returns a confirmation
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations.
  \item The proposer returns the new state to the client.
\end{enumerate}

As you can see, the state transition is almost identical to the Synod's initialization. And if we use "$x \to \mbox{if}\; x = \emptyset \;\mbox{then}\; val\; \mbox{else}\; x$" as the change function then it becomes identical to Synod's attempt to initialize register with the $val$ value.

We may use the following change functions to turn CASPaxos into a distributed compare and set register:
\begin{enumerate}
  \item To {\bf initialize} a register with $val_0$ value: "$x \to \mbox{if}\; x = \emptyset \;\mbox{then}\; (0, val_0)\; \mbox{else}\; x$"
  \item To {\bf update} a register to value $val_1$ if the current version is $5$: "$x \to \mbox{if}\; x = (5, \ast) \;\mbox{then}\; (6, val_1)\; \mbox{else}\; x$"
  \item To {\bf read} a value: "$x \to x$"
\end{enumerate}

With this specialization the protocol becomes almost indistinguishable from Bizur\cite{bizur}.

\subsubsection{One-round trip optimization}

Since the prepare phase doesn't depend on the change function, it's possible to piggyback the next prepare message on the current accept message to reduce the number of round trips from to two to one.

In this case a proposer caches the last written value and the clients should use that proposer to initiate the state transition to benefit from the optimization.

\subsubsection{Cluster membership change}

Cluster membership change is a process of changing the set of processes executing a distributed system without violating safely and liveness properties of the system. It's crucial to have this process because it solves two problems:

\begin{enumerate}
  \item {\it How to change fault tolerance properties of a system}. With time the fault tolerant requirements may change, since a CASPaxos based system of size $N$ tolerates up to $\floor{N/2}$ crashes, a way to increasing/decrease size of a cluster is also a way to increasing/decrease resiliency of the system.

  \item {\it How to replace permanently failed nodes.} CASPaxos tolerates transient failures but when the failures are permanent eventually more than $\floor{N/2}$ nodes crash and the system becomes unavailable. A replacement of a failed node in the $N$ nodes cluster can be modeled as a shrinkage to $N-1$ followed by expansion to $N$.
\end{enumerate}

The procedure is based on Raft's idea of joint consensus\cite{raft} approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.

The proof of this idea with application to CASPaxos is based on two observation:

\begin{enumerate}
  \item {\bf Flexible quorums} It has been observed before that in a Paxos based system the only requirement for the "prepare" and "accept" quorums is intersection \cite{abcds}\cite{vertical}\cite{fpaxos}. For example, if the cluster size is $4$ then we may require $2$ confirmations during the "prepare" phase and $3$ during the "accept" phase. The same result is applicable to CASPaxos, see the proof in appendix \ref{appendix:fpaxos}.
  
  \item {\bf Filter equivalence} If a change in the behaviour of the CASPaxos cluster can be explained by delaying or omitting the messages between the nodes of the cluster then the change doesn't affect consistency since it tolerates the interventions of such kind. It gives freedom in changing the system as long as the change can be modelled as a message filter on top of the unmodified system.
\end{enumerate}

The protocol for changing the set of acceptors from $A_1 \cdots A_{2F+1}$ to $A_1 \cdots A_{2F+2}$:
\begin{enumerate}
  \item Turn on the $A_{2F+2}$ acceptor.
  \item Connect to each proposer and update its configuration to send the accept messages to the $A_1 \cdots A_{2F+2}$ set of acceptors and to require $F+2$ confirmations.
  \item Pick any proposer and execute the identity change function $x \to x$.
  \item Connect to each proposer and update its configuration to send prepare messages to the $A_1 \cdots A_{2F+2}$ set of acceptors and to require $F+2$ confirmations.
\end{enumerate}

From the perspective of the $2F+1$ nodes cluster, the second step can be explained with the filter equivalence principle, so the system keeps being correct. After it finishes the system also works as a $2F+2$ nodes cluster with $F+1$ "prepare" quorum and $F+2$ "accept" quorum.

After read operation finishes the state of the cluster becomes valid from the $F+2$ perspective and we can forget the about the $F+1$ interpretation. The last step switches the system from the reduced "prepare" quorum to the regular.

The protocol for changing the set of acceptors from $A_1 \cdots A_{2F+2}$ to $A_1 \cdots A_{2F+3}$ is simpler because we can treat a $2F+2$ nodes cluster as a $2F+3$ nodes cluster where one node had been down from the beginning:
\begin{enumerate}
  \item Connect to each proposer and update its configuration to send the prepare \& accept messages to the $A_1 \cdots A_{2f+3}$ set of acceptors.
  \item Turn on the $A_{2f+3}$ acceptor.
\end{enumerate}

To reduce the size of the cluster, the same steps should be executed in the reverse order.

\subsubsection{Low-latency and high-throughput consensus across WAN deployments}

WPaxos\cite{wpaxos} paper describes how to achieve low-latency and high-throughput consensus across wide area network through object stealing. It leverages the flexible quorums\cite{fpaxos} idea to cut WAN communication costs. Since CASPaxos being an extension of Synod provides flexible quorums capabilities it can benefit from the same idea too.

\section{A CASPaxos-based key/value storage}

The lightweight nature of CASPaxos opens new simple ways for designing distributed systems with complex behaviour. In this section, we'll discuss a CASPaxos-based design for a key/value storage and compare a research prototype with established databases.

The Raft paper acknoledges\cite{raft} that EPaxos\cite{epaxos} can achieve higher performance than Raft by using leaderless approach and exploiting commutativity in state machine commands. A key/value storage with independent operations between keys looks like a good case for the EPaxos protocol. However, it adds significant complexity.

Alternatively, instead of putting the whole key/value storage under a single RSM and using the commutativity of the commands, we can use lightweight nature of CASPaxos and run a RSM per key to achieving uniform load balancing across all replicas (thus higher throughput).

Gryadka\footnote{\href{https://github.com/gryadka/js}{https://github.com/gryadka/js}} is a prototype of distributed key/value storage which uses that approach.

\subsection{How to delete a record}

The proposed variant of CASPaxos supports only update (change) operation so to delete a value a client should update a register with an empty value. The downside of this approach is the space inefficiency, even when the value is empty the system still spends space to maintain information about the register: a promise and an associated ballot number.

A straightforward removal of this information may introduce consistency issues. Consider the following state of the acceptors.

\begin{figure}[!h]
  \centering
  \begin{tabular}{ r|r|r|r }
    & Promise & Ballot & State \\ \hline
    Acceptor A && 2 & 42 \\
    Acceptor B && 3 & $\emptyset$ \\
    Acceptor C && 3 & $\emptyset$ \\
  \end{tabular}
\end{figure}

According to the CASPaxos protocol a read operation (implemented as $x \to x$ change function) should return $\emptyset$. However, if we decide to remove all the information associated with the register and a request hits the system during the removal when the data on acceptors B and C have already gone then the outcome is $42$ which violates linearizability.

An increasing of the "accept" quorum to $2F+1$ on writing an empty value before the removal solves the problem, but it makes the system less available. A multi-step removal process can be used to avoid the availability impact.

\begin{enumerate}
  \item On a delete request a system writes an empty value with regular $F+1$ accept-quorum, schedules a garbage collection operation and confirms the request to a client.
  \item The garbage collection operation:
  \begin{enumerate}
    \item Replcates an empty value to all nodes by reading it with $2F+1$ quorum size.
    \item Removes the empty register from every acceptor.
  \end{enumerate}
\end{enumerate}

\subsection{Performance}

TODO

\subsection{Fault-tolerance}

TODO: same result as EPaxos \& Bizur.

\section{Related Work}

Raft is similar to CASPaxos in its origin: both were created as an attempt to overcome the complexity of Multi-Paxos. However, Raft uses the same concepts as Multi-Paxos like leader election and log replication but rearranges them differently with focus on understandability. CASPaxos chooses different foundation and has less moving parts. As a result its implementation is $\frac{1}{4}$ of Raft's in terms of lines of code. Another CASPaxos's benefit over Raft is its symmetric peer-to-peer approach: an isolation of a node in CASPaxos cluster doesn't impact client but in Raft an isolation of a leader makes the whole cluster unavailable until new leader isn't choosen.

Bizur is a protocol for building key-value storages, it relates to CASPaxos the same way as Synod does. CASPaxos is a replicated state machine which allows clients to submit functions to change its state. By choosing one set of function we specialize it to be a write-once register, Synod; by choosing another set we get a rewritable register, Bizur. Another contribution is the removal procedure. Bizur deletes values (buckets) by creating a tombstone while CASPaxos removes all associated data leading to better space efficiency.

EPaxos is a leaderless variant of Multi-Paxos which allows concurrent execution of non-interfering commands, CASPaxos is also a leaderless protocol but it doesn't allow concurrent state transition. However in some cases it's comparable with EPaxos. For example, a key-value storage with independent key operations can be modeled as a single EPaxos-based RSM or as a system with an instance of CASPaxos per key. Both achieve optimal commit latency, uniform load balancing across all replicas and graceful performance degradation when replicas are slow or crash.

\section{Conclusion}

Desprite log based paxos like consensus protocols are complex and have latency issues during failures they find wide adoption in the industry as a foundation of key. However, when looking closely at the applications many of them are used to implement master-master replicated key-value storages.

CASPaxos is the consensus protocol which overcomes the complexity of log based solutions like Multi-Paxos and Raft and uses symmetric peer to peer approach to avoid the latency issues. The lightweight nature of CASPaxos allows to use it in a key-value storage design wich it simple than EPaxos but provides comparable resiliency guarantees.

The comparative benchmarking with MongoDB and Etcd has shown that CASPaxos is comparable in regular operation and outperforms

The protocol has formal proof, was model checked with TLA+ and a implementation was tested using fault-injection methodology.

\begin{appendices}
\section{Proof}
\label{appendix:proof}

\begin{theorem} \label{th:proof}
  We want to prove that for any two acknoledged changes one is always a descendant of another.
  
  Let $\bar{E}^2$ represent a set of acknoledged events (a proposer received at least $F+1$ confirmations of an "accept" message) and $\to$ represent the "is a descendant" relation, then we want to demonstrate that

  \begin{equation}
    \forall x,y \in \bar{E}^2 \;:\; x \to y \lor y \to x
  \end{equation}
\end{theorem}

\theoremstyle{definition}
\begin{definition}{\bf("is a descendant" relation)}
  What does it mean that one event is a descendant of another? Informally it means that there is a chain of changes changing one state into another. Let's formalize it. We start by defining this relation between the succesfuly accepted messages (denoted as $\ddot{E}^2$) and then expend it to $\bar{E}^2$.

  By definition of a proposer any accepted state is a function of previously accepted state, so
  
  \begin{equation} \label{eq:chain}
    \forall x \in \ddot{E}^2 \quad \exists ! f \quad \exists ! y \in \ddot{E}^2 : s(x) = f(s(y))
  \end{equation}
  
  Where $s(x)$ is a state accepted by an acceptor resulting in event $x$. When \ref{eq:chain} holds for $x$ and $y$ we write $y \sim x$ and $y = I^{-1}(x)$. Now we can define "is a descendant" relation between $\ddot{E}^2$ events as:
  
  \begin{equation}
    \forall x \in \ddot{E}^2 \; \forall x \in \ddot{E}^2 \;:\; x \to y \equiv x \sim y \lor (\exists z \in \ddot{E}^2 \;:\; x \to z \land z \to y)
  \end{equation}
  
  Let's define $x.w$ for $x \in \bar{E}^2$ as events which correspond to the confirmations of an accept message. By definition the following properties are true:
  \begin{enumerate}
    \item $\forall x \in \bar{E}^2 \; x.w \subset \ddot{E}^2$
    \item $\forall x \in \bar{E}^2 \; |x.w| >= F+1$ (we require a quorum of confirmations before acknoledging a change)
    \item $\forall x \in \bar{E}^2 \; \forall y \in x.w \;:\; s(x) = s(y)$ (accepted state and acknoledged state is the same)
  \end{enumerate}
  
  The third property allows to continue "is a descendant" relation on $\bar{E}^2$:
  
  \begin{equation}
    \forall x \in \bar{E}^2 \; \forall x \in \bar{E}^2 \;:\; x \to y \equiv (\forall a \in x.w \; \forall b \in y.w \; a \to b)
  \end{equation}
\end{definition}

\begin{lemma}
  The following statement proves the theorem \ref{th:proof}.

  \begin{equation} \label{eq:step}
    \forall x \in \bar{E}^2 \; \forall y \in \ddot{E}^2 \;:\; x.b < y.b \implies x.b \leq I^{-1}(y).b
  \end{equation}

  Where $x.b$ means a ballot number of an acknoledment or confirmation depending on type $x$.
\end{lemma}

\begin{proof}
  Let $z_0 = y$ and $z_{n+1} = I^{-1}(z_{n})$. By definition ballot numbers only increase: $z_{n+1}.b < z_{n}.b$, so we can use mathematical induction and \ref{eq:step} guarantees that $\exists k \;:\; z_k.b = x.b$ meaning $s(z_k) = s(x)$. Since $z_{k+1} \sim z_k$ we proved the following statement:

  \begin{equation} \label{eq:bd}
    \forall x \in \bar{E}^2 \; \forall y \in \ddot{E}^2 \;:\; x.b < y.b \implies x \to y
  \end{equation}

  Since $\forall y \in \bar{E}^2 \; \forall z \in y.w \;:\; y.b=z.b \land s(y)=s(z)$ then \ref{eq:bd} implies

  \begin{equation}
    \forall x \in \bar{E}^2 \; \forall y \in \bar{E}^2 \;:\; x.b < y.b \implies x \to y
  \end{equation}

  By definition, $\forall x \in \bar{E}^2 \; \forall y \in \bar{E}^2 \;:\; x.b < y.b \lor y.b < x.b$ so the latter means

  \begin{equation}
    \forall x \in \bar{E}^2 \; \forall y \in \bar{E}^2 \;:\; x \to y \lor y \to x
  \end{equation}

  Which proofs the theorem \ref{th:proof}.
\end{proof}

Before proving the \ref{eq:step} let's define $\ddot{E}^1$ as a set of promised events (prepare phase) and $x.r$ as for $x \in \ddot{E}^2$ as events which correspond to the confirmations of an prepare message. By definition the following properties hold:
\begin{enumerate}
  \item $\forall x \in \ddot{E}^2 \; x.r \subset \ddot{E}^1$
  \item $\forall x \in \ddot{E}^2 \; |x.r| >= F+1$ (we require a quorum of confirmations before staring the accept phase)
\end{enumerate}.

\begin{theorem} \label{th:proof2}
  $$\forall x \in \bar{E}^2 \; \forall y \in \ddot{E}^2 \;:\; x.b < y.b \implies x.b \leq I^{-1}(y).b$$
\end{theorem}

\begin{proof}
  Let
  
  $$N = \{z.node \;|\; z \in x.w\} \cap \{z.node \;|\; z \in y.r\}$$

  $N$ isn't empty because $x.w$ intersects with $y.r$. Let $n \in N$, and $w \equiv x.w |_n$ and $u \equiv y.r |_n$ are accepted and promised events on node $n$. We know that event $w$ happened before $u$ in $n$'s timeframe because an acceptor doesn't accept messages with lesser ballot numbers than they already saw and $w.b = x.b < y.b = u.b$ implies

  \begin{equation}
    w.b < u.b
  \end{equation}

  Let "promise" events $P$ have the following structure $\{ \mbox{ts}, \mbox{b}, \mbox{ret} : \{ \mbox{b}, \mbox{s} \} \}$ where ts - local time, b - promised ballot number, ret.b - a ballot number of the accepted state and ret.s is the state itself. "accept" events $A$'s structure is  $\{ \mbox{ts}, \mbox{b}, \mbox{s} \}$.

  By definition "promise" confirmation returns the most latest state, let's write it formaly

  \begin{equation} \label{eq:last}
    \forall k \in P \; k.ret.b = \max \{ l.b \;|\;l \in A \land l.ts < k.ts \} \\
  \end{equation}

  Since $w.b < u.b$, $w \in A$ and $u \in P$

  \begin{equation}
    w \in \{ z \in A, z.ts < u.ts \}
  \end{equation}

  With combination with \ref{eq:last} it implies:

  \begin{equation} \label{eq:final}
    x.b = w.b \leq \max \{ z \in A, z.ts < u.ts \} = u.ret.b
  \end{equation}

  By definition we pick a state with max ballot number out of quorum of promise confirmations as current state so:

  \begin{equation}
    I^{-1}(y).b = \max \{ z.ret.b | z \in y.r \}
  \end{equation}

  Combining with \ref{eq:final} we get:

  \begin{equation}
    x.b = w.b \leq \max \{ z \in A, z.ts < u.ts \} = u.ret.b \leq \max \{ z.ret.b | z \in y.r \} = I^{-1}(y).b
  \end{equation}

  Which proves $x.b \leq I^{-1}(y).b$.

\end{proof}

\section{FPaxos}
\label{appendix:fpaxos}
In the proof of CASPaxos \ref{appendix:proof} we didn't use the size of the promise/accept quorums and only used thier intersection, so the same proof proves FPaxos observation too.

\end{appendices}

\newpage

\begin{thebibliography}{9}

\bibitem{lamport01}
  Leslie Lamport,
  \emph{"Paxos Made Simple"}.
  2001.

\bibitem{abcds}
  Butler W. Lampson
  \emph{"The ABCDs of Paxos"}.
  2001.

\bibitem{chubby}
  Tushar Chandra, Robert Griesemer, Joshua Redstone
  \emph{"Paxos Made Live - An Engineering Perspective"}.
  2007.

\bibitem{vertical}
  Leslie Lamport, Dahlia Malkhi, Lidong Zhou
  \emph{"Vertical Paxos and Primary-Backup Replication"}
  2009.

\bibitem{spanner}
  Corbett, J. C., Dean, J., Epstein, M., Fikes, A., Frost C., Furman, J.J., Ghemawat, S., Gubarev, A., Heiser, C., Hochschild, P., at al.
  \emph{"Spanner: Googles globally distributed database"}.
  2012.

\bibitem{raft}
  Diego Ongaro, John Ousterhout
  \emph{"In Search of an Understandable Consensus Algorithm"}.
  2013.

\bibitem{epaxos}
  Iulian Moraru, David G. Andersen, Michael Kaminsky
  \emph{"There Is More Consensus in Egalitarian Parliaments"}.
  2013.

\bibitem{fpaxos}
  Heidi Howard, Dahlia Malkhi, Alexander Spiegelman
  \emph{"Flexible Paxos: Quorum intersection revisited"}.
  2016.

\bibitem{wpaxos}
  Ailidani Ailijiang, Aleksey Charapko, Murat Demirbas, Tevfik Kosar
  \emph{"WPaxos: Ruling the Archipelago with Fast Consensus"}.
  2017.

\bibitem{bizur}
  Ezra N. Hoch, Yaniv Ben-Yehuda, Noam Lewis, Avi Vigder
  \emph{"Bizur: A Key-value Consensus Algorithm for Scalable File-systems"}.
  2017.

\end{thebibliography}

\end{document}