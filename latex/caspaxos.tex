\documentclass[12pt]{article}

\usepackage[toc,page]{appendix}

\usepackage[utf8]{inputenc}

\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{enumerate}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\small}

\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{Liberation Serif}
\setsansfont{Liberation Sans}
\setmonofont{Liberation Mono}

\usepackage{mathtools}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\usepackage{polyglossia}
\setdefaultlanguage{english}

\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref}

\begin{document}

\title{CASPaxos: Replicated State Machines without logs}
\date{30 January 2018}
\author{Denis Rystsov \\
\href{mailto:rystsov.denis@gmail.com}{rystsov.denis@gmail.com}}


\maketitle

\begin{abstract}
CASPaxos is a Single Decree Paxos (Synod) based protocol for building a replicated state machine. Unlike Raft and Multi-Paxos, it doesn't use an idea of the leader election and the replicated log, so it avoids associated complexity (e.g. log compaction) and provides graceful performance degradation when replicas are crashed.

The lightweight nature of CASPaxos opens ways for a new application for RSM, e.g. instead of representing a key/value storage as a single RSM; it's possible to run an instance of CASPaxos per key and achieve a result similar to EPaxos.

This paper describes CASPaxos protocol, proves its safety properties, covers cluster membership change and evaluates a CASPaxos-based key/value storage against established consistent databases.
\end{abstract}


\section{Introduction}

Multi-Paxos\cite{lamport01} and Raft\cite{raft} protocols allow a collection of machines to work as a coherent group - a replicated state machine (RSM). The protocols preserve liveness when at least $\floor{N/2} + 1$ of $N$ machines are up and connected, preserve safety in the presence of arbitrary crash/recovery and loss of messages.

The problem of keeping RSM work when its nodes are falling apart is isomorphic to the problem of master-master replication of a linearizable distributed storage under the same conditions. So those protocols are widely used in the industry as a foundation of such databases as Chubby\cite{chubby}, Etcd\footnote{\href{https://github.com/coreos/etcd}{https://github.com/coreos/etcd}}, Spanner\cite{spanner}, etc.

Despite the wide adoption, there are a lot of indications that the protocols are complex. Diego Ongaro and John Ousterhout, authors of Raft, wrote in their paper:

\begin{quote}
In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers. We struggled with Paxos ourselves; we were not able to understand the complete protocol until after reading several simplified explanations and designing our own alternative protocol, a process that took almost a year
\end{quote}

Google's engineers wrote about their experience of building a Paxos-based database in the "Paxos Made Live"\cite{chubby} paper:

\begin{quote}
Despite the existing literature in the field, building such a database proved to be non-trivial \ldots{} While Paxos can be described with a page of pseudo-code, our complete implementation contains several thousand lines of C++ code \ldots{} There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.
\end{quote}

The complexity of RSM protocols may lead to errors in implementations. Kyle Kingsbury made a comprehensive research\footnote{\href{https://aphyr.com/tags/jepsen}{https://aphyr.com/tags/jepsen}} of distributed consistent databases and found violations of linearizability in almost every database he tested including MongoDB, Etcd, Consul, RethinkDB, VoltDB and CockroachDB.

{\bf Contributions.} I present CASPaxos, a novel protocol for building RSM that avoids complexities of Multi-Paxos and Raft. Multi-Paxos is RSM built on top of a replicated log in which each log entry is a command. The replicated log is modelled as an array of instances of Single Decree Paxos. According to D. Ongaro and J. Ousterhout its complexity comes from the composition rules:

\begin{quote}
We hypothesize that Paxosâ€™ opaqueness derives from its choice of the single-decree subset as its foundation \ldots{} The composition rules for Multi-Paxos add significant additional complexity and subtlety.

One reason is that there is no widely agreed upon algorithm for multi-Paxos. Lamportâ€™s descriptions are mostly about single-decree Paxos; he sketched possible approaches to multi-Paxos, but many details are missing. As a result, practical systems bear little resemblance to Paxos. Each implementation begins with Paxos, discovers the difficulties in implementing it, and then develops a significantly different architecture \ldots{} real implementations are so different from Paxos that the proofs have little value
\end{quote}

CASPaxos extends Synod, a write-once distributed register, into a rewritable distributed register (which is isomorphic to a RSM) instead of using it as a building block, so there is no composition and the associated complexity.

According to an experimental study\cite{raft} and the number of open source implementations\footnote{\href{https://raft.github.io/\#implementations}{https://raft.github.io/\#implementations}} Raft is more understandable than Multi-Paxos. However, its complexity is comparable with the latter: both systems\cite{chubby}\cite{raft} have several thousand of lines of code, both use concepts of leader, leader election, leases, both are based on logs and require log compaction. CASPaxos is significantly simpler: it doesn't have those pieces and its implementation\footnote{\href{https://github.com/gryadka/js}{https://github.com/gryadka/js}} is less than 500 lines of code.

Being just an extension of Synod, CASPaxos uses its symmetric peer-to-peer approach and automatically achieves the goals set in the EPaxos\cite{epaxos} paper: (1) optimal commit latency in the wide-area when tolerating one and two failures, under realistic conditions; (2) uniform load balancing across all replicas (thus achieving high throughput); and (3) graceful performance degradation when replicas are slow or crash.

The formal proof is included into the appendix, TLA+ models were independently written by Tobias Schottdorf\footnote{\href{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}} and Greg Rogers\footnote{\href{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}} and they didn't reveal any linearizability violation.

In the following sections, I describe the CASPaxos protocol, cluster membership change and evaluate a CASPaxos-based key/value storage.

\section{Algorithm}

We begin by briefly describing the Synod protocol from the perspective of master-master replication, followed by an overview of its extension into CASPaxos.

\subsection{Synod}

Synod protocol allows to build a distributed register which a client can initialize only once. If several clients try to initialize a register concurrently then the requests either prevent each other from continuing or a single initialization succeeds. Once a client recieved confirmation all the follow up initializations must result with a conflict and return the initialized value.

The system belongs to the CP-specter of CAP theorem and keeps working without compromising safety when at least $\floor{N/2} + 1$ of $N$ nodes are up and connected; with more failures it compromises availability.

The roles of nodes in the system:
\begin{enumerate}
  \item {\bf Clients} initiate a request by communicating with a proposer, client may be stateless, the system may have arbitrary numbers of clients.
  \item {\bf Proposers} perform the initialization by communicating with acceptors. Proposers keep state to generate unique increasing update ID (ballot number), the system may have arbitrary numbers of proposers.
  \item {\bf Acceptors} store the accepted value, the system should have $2F+1$ acceptor to tolerate $F$ failures.
\end{enumerate}

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[y=-1cm]
    \node at (0,-0.5)[scale=0.8]{Client};
    \node at (2,-0.5)[scale=0.8]{Proposer};
    \node at (4.5,-0.5)[scale=0.8]{Acceptor A};
    \node at (6.5,-0.5)[scale=0.8]{Acceptor B};
    \node at (8.5,-0.5)[scale=0.8]{Acceptor C};
    
    \draw (0,0) -- (0,5.6);
    \draw (2,0) -- (2,5.6);
    \draw (4.5,0) -- (4.5,5.6);
    \draw (6.5,0) -- (6.5,2.05);
    \draw (8.5,0) -- (8.5,5.6);
    \draw (6.4,2.05) -- (6.6,2.05);
  
    \draw[dotted] (-0.2,0.75) -- (8.6,0.75);
    \draw[dotted] (-0.2,2.55) -- (8.6,2.55);
    \draw[dotted] (-0.2,4.05) -- (8.6,4.05);
  
    \begin{scope}[very thick]
      \draw[->] (0,0.5) -- (2,0.5) node[above, midway, scale=0.8]{set 3};
      
      \node at (1,1.6)[scale=0.8]{Propose};
      \draw[->] (2,1) -- (4.5,1);
      \draw[->] (2,1.3) -- (6.5,1.3);
      \draw[->] (2,1.6) -- (8.5,1.6);
      \draw[<-, dashed] (2,1.9) -- (4.5,1.9);
      \draw[<-, dashed] (2,2.2) -- (8.5,2.2);
  
      \node at (1,3.35)[scale=0.8]{Accept};
      \draw[->] (2,2.9) -- (4.5,2.9);
      \draw[->] (2,3.2) -- (8.5,3.2);
      \draw[<-, dashed] (2,3.5) -- (4.5,3.5);
      \draw[<-, dashed] (2,3.8) -- (8.5,3.8);
  
      \draw[<-, dashed] (0,4.3) -- (2,4.3) node[below, midway, scale=0.8]{ok};
    \end{scope}
  \end{tikzpicture}
\end{figure}

The initialization procedure:
\begin{enumerate}
  \item A client proposes a value to a proposer.
  \item The proposer generates a ballot number, $B$, and send a prepare message containing that number to the acceptors.
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Persists the ballot number as promise and returns a confirmation either with an empty value (if it hasn't accepted any yet) or with a tuple of an accepted value and its ballot number
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \begin{itemize}
    \item If they all contain an empty value then the proposer writes "ok" to the result variable and sends an accept message containing the ballot number $B$ and the proposing value to the acceptors.
    \item If at least one message contains a tuple then the proposer picks the tuple with the highest ballot number, writes ("conflict", the tuple's value) to the result variable and sends an accept message with the generated ballot number $B$ and the tuple's value to the acceptors.
  \end{itemize}
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Erases the promise, marks the recieved (ballot number, value) as the accepted value and returns a confirmation
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \item The proposer returns the result variable to the client
\end{enumerate}

\subsection{CASPaxos}
\subsubsection{1-round trip optimisation}
\subsubsection{Cluster membership change}
\subsubsection{WPaxos}
\section{Evaluation}
\section{Conclusion}

\begin{appendices}
\section{Proof}
\href{http://rystsov.info/2015/09/16/how-paxos-works.html}{http://rystsov.info/2015/09/16/how-paxos-works.html}
\section{FPaxos}
\end{appendices}

\newpage

\begin{thebibliography}{9}

\bibitem{lamport01}
  Leslie Lamport,
  \emph{"Paxos Made Simple"}.
  2001.

\bibitem{raft}
  Diego Ongaro, John Ousterhout
  \emph{"In Search of an Understandable Consensus Algorithm"}.
  2013.

\bibitem{epaxos}
  Iulian Moraru, David G. Andersen, Michael Kaminsky
  \emph{"There Is More Consensus in Egalitarian Parliaments"}.
  2013.

\bibitem{chubby}
  Tushar Chandra, Robert Griesemer, Joshua Redstone
  \emph{"Paxos Made Live - An Engineering Perspective"}.
  2007.

\bibitem{spanner}
  Corbett, J. C., Dean, J., Epstein, M., Fikes, A., Frost C., Furman, J.J., Ghemawat, S., Gubarev, A., Heiser, C., Hochschild, P., at al.
  \emph{"Spanner: Googles globally distributed database"}.
  2012.

\bibitem{fpaxos}
  Heidi Howard, Dahlia Malkhi, Alexander Spiegelman
  \emph{Flexible Paxos: Quorum intersection revisited}.
  2016.

\end{thebibliography}

\end{document}